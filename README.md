# Data modeling with Postgres for Sparkify music streaming service
# By Mostafa Mohamed Mohamed Imam

<br>
<br>

## Project introduction

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They'd like a data engineer to create a Postgres database with tables designed to optimize queries on song play analysis, and bring you on the project. Your role is to create a database schema and ETL pipeline for this analysis. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

<br>

## Requirements

- python 3
- psycopg2
- pandas
- localy hosted PostgreSQL database

<br>

## Project datasets

### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
```
data/song_data/A/B/C/TRABCEI128F424C983.json
data/song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
```
{
    "num_songs": 1,
    "artist_id": "ARJIE2Y1187B994AB7",
    "artist_latitude": null,
    "artist_longitude": null,
    "artist_location": "",
    "artist_name": "Line Renaud",
    "song_id": "SOUPIRU12A6D4FA1E1",
    "title": "Der Kleine Dompfaff",
    "duration": 152.92036,
    "year": 0
}
```
<br>

### Log Dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.  

The log files in the dataset we'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
data/log_data/2018/11/2018-11-12-events.json
data/log_data/2018/11/2018-11-13-events.json
```
Below is an example of what the data in a log file, 2018-11-12-events.json, looks like.
![log-data](https://video.udacity-data.com/topher/2019/February/5c6c15e9_log-data/log-data.png)

<br>
<br>

## Project describtion

- In this project we build an ETL pipeline to transform json records and logs into more meaningful and well structured form... a postgres schema containing 5 tables

    - songplays : contains song streaming data such as
        - the time of streaming
        - song name
        - song and artist ids if matched with the stored songs and artists in their respective tables
        - session ids
        - user agents (info about the browser and os of the user device).
      
    <br>
    - users : contains data about users such as
        - their names
        - gender
        - streaming tier (free, paid)
    
    <br>
    - songs : contains data about songs such as
        - titles
        - song ids
        - artist ids for the artist associated with the song
        - release year
        - song duration
    
    <br>
    - artists : contains data about artists such as
        - names
        - artist ids
        - artist location
    
    <br>
    - time : contains data about streaming session times (start time, day, week day, week, month, year)
    
<img src="star schema.png" alt="star schema" title="schema">
<br>

- Song file names are extracted by looping over the song data sub directories in the data directory and getting a list of the names of all included files.
<br>

- The list of file names is used to loop over the files and write their data to one main json file `all_songs.json`.
<br>

- The json file is imported into a dataframe then song and artist data are extracted from it, checked, cleaned and then saved in csv files (`songs.csv`, `artists.csv`) and then copied from the csv files and inserted in bulk into their respective database tables using the `copy` sql command which is much more efficient than using single insert queries in a for loop.
<br>

- The log files data is extracted combined in a dataframe by looping over all log files in log data sub directories in the data directory and appending their content to a dataframe.
<br>

- User data and time data are extracted from the log data, checked, cleaned and saved in csv files (`users.csv`, `time.csv`) and then copied from the csv files and inserted in bulk into their respective database tables using the `copy` sql command which is much more efficient than using single insert queries in a for loop.
<br>

- The songplays data (minus the song ids and artist ids) is extracted from the log data and saved in a csv file `records.csv`
<br>

- The incomplete (without song ids and artist ids) songplays records data is copied from the csv file and inserted in bulk into a temporary database table and then the full songplays data is inserted in bulk into the songplays table from a left outer join of the records table and the songs table to join on matching song names and still keep the rest of the records without matching songs stored in the songs table.
<br>

## Project file structure

- data : the directory containing the song data and log data.
<br>

- create_tables.py : a python script file used to create the database and the database tables using queries from `sql_queries.py`, it is also used drop everything and restart the process.
<br>

- etl.ipynb : a python jupyter notebook that is used to initially explore, check and transform the data.
<br>

- etl.py : a python script used to perform ETL on the json data
<br>

- sql_queries.py : a python file containing sql queries to create the schema tables, insert data into the tables, bulk insert into the tables
<br>

- test.ipynb : a python jupyter notebook that is used to test the success of the ETL process as well as asses the quality of the database tables
<br>

- README.md : the current file, a readme file documenting the project.

<br>
<br>

## project instructions

1. Run `create_tables.py` script in the terminal by typing the command ```python create_tables.py```.
<br>

2. Run `etl.py` script in the terminal by typing the command ```python etl.py```.
<br>

3. Open `test.ipynb` notebook and run the code cells to check the success of the `etl.py` script.